{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing usps from pickle file .....\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import utils\n",
    "import matplotlib\n",
    "\n",
    "dataPath = './data/'\n",
    "def prepare_usps_mlfetch():\n",
    "\n",
    "    import tempfile\n",
    "    import pickle\n",
    "    print \"importing usps from pickle file .....\"\n",
    "\n",
    "    with open(dataPath+'usps_data.pkl', \"rb\") as fp:\n",
    "          loaded_data1 = pickle.load(fp)\n",
    "\n",
    "    # test_data_home = tempfile.mkdtemp()\n",
    "    # from sklearn.datasets.mldata import fetch_mldata\n",
    "    # usps = fetch_mldata('usps', data_home=test_data_home)\n",
    "    # print usps.target.shape\n",
    "    # print type(usps.target)\n",
    "    labels = loaded_data1['target']\n",
    "    data = loaded_data1['data']\n",
    "    # print \"******\",labels\n",
    "\n",
    "    k_ones = np.where(labels == 2)\n",
    "    label_ones = labels[k_ones]\n",
    "    data_ones = data[k_ones]\n",
    "\n",
    "    k_sevens = np.where(labels == 8)\n",
    "    label_sevens = labels[k_sevens]\n",
    "    data_sevens = data[k_sevens]\n",
    "    #\n",
    "    # print \"data_sevens:\",data_sevens.shape\n",
    "    # print \"label_sevens:\",label_sevens.shape\n",
    "    # print \"data_ones:\",data_ones.shape\n",
    "    # print \"label_ones:\",label_ones.shape\n",
    "    #\n",
    "    data_ones = data_ones[:220]\n",
    "    label_ones= label_ones[:220]\n",
    "    data_sevens = data_sevens[:11]\n",
    "    label_sevens = label_sevens[:11]\n",
    "\n",
    "    data = np.concatenate((data_ones,data_sevens),axis=0)\n",
    "    label = np.concatenate((label_ones,label_sevens),axis=0)\n",
    "    label[0:220] = 1\n",
    "    label[220:231] = -1\n",
    "    # print \"1-s\",data[0]\n",
    "    # print label\n",
    "    # print \"7-s\",data[230]\n",
    "    # print label\n",
    "    # print \"data:\",data.shape\n",
    "    # print \"label:\",label.shape\n",
    "    return [data,label]\n",
    "\n",
    "[Xtrue,Xlabels] = prepare_usps_mlfetch()\n",
    "data = Xtrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrBAOfDOzQjjwzcwK8V//VvfepQF7EgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c5627d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's take a look at the types of labels  are present in the data.\n",
    "# The ones correspond to label 1 and 7's(outliers) correspond to label -1\n",
    "#data.label.value_counts().plot(kind='bar')  \n",
    "type(Xlabels)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Xlabels,bins=5)\n",
    "plt.title(\"Count of Normal and Anomalous datapoints in training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAKING OUR DATA ONE-CLASS\n",
    "\n",
    "Later we're going to use scikit-learn's OneClassSVM predict function to generate output. This returns +1 or -1 to indicate whether the data is an \"inlier\" or \"outlier\" respectively. To make comparison easier later we'll replace our data's label with a matching +1 or -1 value. This also transforms our data from multi-class (multiple different labels) to one-class (boolean label), which is a prerequisite for using a one-class SVM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('outliers.shape', (11,))\n",
      "('outlier fraction', 0.047619047619047616)\n",
      "Training data shape... (231, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# series, not a new dataframe\n",
    "target = Xlabels\n",
    "# find the proportion of outliers we expect (aka where `labels == -1`). because \n",
    "# target is a series, we just compare against itself rather than a column.\n",
    "outliers = target[target == -1]  \n",
    "print(\"outliers.shape\", outliers.shape)  \n",
    "print(\"outlier fraction\", float(outliers.shape[0])/target.shape[0])\n",
    "\n",
    "# Print the shape of the input data for sanity\n",
    "print \"Training data shape...\",data.shape\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLITTING DATA INTO TRAINING AND TEST SETS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_target, test_target = train_test_split(data, target, train_size = 0.8)  \n",
    "train_data.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING THE MODEL\n",
    "\n",
    "Now we're ready to train our model. We do this by calling the fit function from scikit-learn's svm.OneClassSVM. It accepts a few parameters but the most important are nu, kernel, and for the RBF kernel we'll be using, gamma.\n",
    "\n",
    "nu is \"An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors\" and must be between 0 and 1. Basically this means the proportion of outliers we expect in our data. This is an important factor to consider when assessing algorithms. Many unsupervised ML algorithms require you to know (or hint at) the number of outliers or class members you expect.\n",
    "kernel is the kernel type to be used. Earlier we discussed SVM's ability to use a non-linear function to project the hyperspace to higher dimension. Setting kernel to something other than linear here will achieve that. The default is rbf (RBF - radial basis function).\n",
    "gamma is a parameter of the RBF kernel type and controls the influence of individual training samples - this effects the \"smoothness\" of the model. A low value improves the smoothness and \"generalizability\" of the model, while a high value reduces it but makes the model \"tighter-fitted\" to the training data. Some experimentation is often required to find the best value.\n",
    "We already know that the proportion of attacks in our data is about 4%. We'll get the precise fraction and use that for nu below. Through experimentation I found an effective gamma to be 0.00005.\n",
    "\n",
    "In our next cell we'll instantiate a model and fit (train) it with our training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nu', 0.047619047619047616)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=5e-05, kernel='rbf',\n",
       "      max_iter=-1, nu=0.047619047619, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# set nu (which should be the proportion of outliers in our dataset)\n",
    "nu = float(outliers.shape[0]) / target.shape[0]  \n",
    "print(\"nu\", nu)\n",
    "\n",
    "model = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.00005)  \n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECKING ACCURACY OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training Dataset Accuracy=======================\n",
      "('Training Dataset accuracy: ', 0.95652173913043481)\n",
      "('Training Dataset precision: ', 0.98843930635838151)\n",
      "('Training Dataset recall: ', 0.96610169491525422)\n",
      "('Training Dataset f1: ', 0.97714285714285709)\n",
      "('Training Dataset area under curve (auc): ', 0.84019370460048426)\n",
      "====== Test Dataset Accuracy=======================\n",
      "('Test Dataset accuracy: ', 0.91489361702127658)\n",
      "('Test Dataset precision: ', 0.97560975609756095)\n",
      "('Test Dataset recall: ', 0.93023255813953487)\n",
      "('Test Dataset f1: ', 0.95238095238095244)\n",
      "('Test Dataset area under curve (auc): ', 0.84011627906976738)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "preds = model.predict(train_data)  \n",
    "targs = train_target\n",
    "print (\"====== Training Dataset Accuracy=======================\")\n",
    "print(\"Training Dataset accuracy: \", metrics.accuracy_score(targs, preds))  \n",
    "print(\"Training Dataset precision: \", metrics.precision_score(targs, preds))  \n",
    "print(\"Training Dataset recall: \", metrics.recall_score(targs, preds))  \n",
    "print(\"Training Dataset f1: \", metrics.f1_score(targs, preds))  \n",
    "print(\"Training Dataset area under curve (auc): \", metrics.roc_auc_score(targs, preds))  \n",
    "print (\"====== Test Dataset Accuracy=======================\")\n",
    "\n",
    "preds = model.predict(test_data)  \n",
    "targs = test_target\n",
    "\n",
    "print(\"Test Dataset accuracy: \", metrics.accuracy_score(targs, preds))  \n",
    "print(\"Test Dataset precision: \", metrics.precision_score(targs, preds))  \n",
    "print(\"Test Dataset recall: \", metrics.recall_score(targs, preds))  \n",
    "print(\"Test Dataset f1: \", metrics.f1_score(targs, preds))  \n",
    "print(\"Test Dataset area under curve (auc): \", metrics.roc_auc_score(targs, preds))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
