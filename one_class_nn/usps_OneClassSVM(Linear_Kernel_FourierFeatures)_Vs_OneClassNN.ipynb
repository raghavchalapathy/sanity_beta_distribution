{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing usps from pickle file .....\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from data_fetch import prepare_usps_mlfetch\n",
    "[Xtrue,Xlabels] = prepare_usps_mlfetch()\n",
    "data = Xtrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAITCAYAAAByoWiiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXEgC9JkiQ15P8HWQvewMlrsiEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c96fd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 265,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "# let's take a look at the types of labels  are present in the data.\n",
    "# The ones correspond to label 1 and 7's(outliers) correspond to label -1\n",
    "#data.label.value_counts().plot(kind='bar')  \n",
    "type(Xlabels)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Xlabels,bins=5)\n",
    "plt.title(\"Count of Normal and Anomalous datapoints in training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAKING OUR DATA ONE-CLASS\n",
    "\n",
    "Later we're going to use scikit-learn's OneClassSVM predict function to generate output. This returns +1 or -1 to indicate whether the data is an \"inlier\" or \"outlier\" respectively. To make comparison easier later we'll replace our data's label with a matching +1 or -1 value. This also transforms our data from multi-class (multiple different labels) to one-class (boolean label), which is a prerequisite for using a one-class SVM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('outliers.shape', (11,))\n",
      "('outlier fraction', 0.047619047619047616)\n",
      "Training data shape... (231, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# series, not a new dataframe\n",
    "target = Xlabels\n",
    "# find the proportion of outliers we expect (aka where `labels == -1`). because \n",
    "# target is a series, we just compare against itself rather than a column.\n",
    "outliers = target[target == -1]  \n",
    "print(\"outliers.shape\", outliers.shape)  \n",
    "print(\"outlier fraction\", float(outliers.shape[0])/target.shape[0])\n",
    "\n",
    "# Print the shape of the input data for sanity\n",
    "print \"Training data shape...\",data.shape\n",
    "# temp =  data[0:180]\n",
    "# print temp.shape\n",
    "# print data[1]\n",
    "# data = data[0:220]\n",
    "# target = target[0:220]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLITTING DATA INTO TRAINING AND TEST SETS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import train_test_split\n",
    "# train_data, test_data, train_target, test_target = train_test_split(data, target, train_size = 0.8)  \n",
    "# train_data.shape  \n",
    "\n",
    "\n",
    "# # We learn the digits on the first half of the digits\n",
    "# data_train, targets_train = train_data,train_target\n",
    "\n",
    "# # Now predict the value of the digit on the second half:\n",
    "# data_test, targets_test = test_data,test_target\n",
    "\n",
    "data_train = data[0:220]\n",
    "targets_train = target[0:220]\n",
    "data_test = data[220:231]\n",
    "targets_test = target[220:231]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nu', 0.047619047619047616)\n",
      "One Class SVM output: for negative values are\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "Decision Scores:\n",
      "[[-538.08467136]\n",
      " [-842.87880859]\n",
      " [-310.50698618]\n",
      " [-384.24332765]\n",
      " [-628.43266278]\n",
      " [-408.94578019]\n",
      " [-444.36691608]\n",
      " [-443.12525508]\n",
      " [-554.60242858]\n",
      " [-536.11272806]\n",
      " [-663.17592883]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvsAAAITCAYAAACZltOhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aead650>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 265,
       "width": 381
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# set nu (which should be the proportion of outliers in our dataset)\n",
    "nu = float(outliers.shape[0]) / target.shape[0]  \n",
    "print(\"nu\", nu)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, pipeline\n",
    "from sklearn.kernel_approximation import (RBFSampler,\n",
    "                                          Nystroem)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics \n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "kernel_svm = svm.SVC(gamma=.2)\n",
    "linear_svm = svm.LinearSVC()\n",
    "oneClass_svm = svm.OneClassSVM(nu=nu, kernel='linear', gamma=0.00005)\n",
    "oneClass_svm_time = time()\n",
    "oneClass_svm.fit(data_train) \n",
    "preds = oneClass_svm.predict(data_test)  \n",
    "decisionScore= oneClass_svm.decision_function(data_test)\n",
    "decisionScoreTrain= oneClass_svm.decision_function(data_train)\n",
    "print \"One Class SVM output: for negative values are\"\n",
    "print preds\n",
    "print \"Decision Scores:\"\n",
    "print decisionScore\n",
    "\n",
    "# plt.hist(decisionScore);\n",
    "# plt.hist(decisionScoreTrain);\n",
    "\n",
    "plt.hist(decisionScoreTrain, label='Normal');\n",
    "plt.hist(decisionScore, label='Anomalies');\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('OneClass SVM Decision Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Single Hidden Layer : One ClassNeural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.00019\u001b[0m\u001b[0m | time: 0.008s\n",
      "| SGD | epoch: 200 | loss: 0.00019 - binary_acc: 0.0000 -- iter: 192/220\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.00019\u001b[0m\u001b[0m | time: 0.010s\n",
      "| SGD | epoch: 200 | loss: 0.00019 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Running Iteration : 0\n",
      "---------------------------------\n",
      "Run id: 4MJSSE\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.00019\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 201 | loss: 0.00019 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.00019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 202 | loss: 0.00019 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.00019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 203 | loss: 0.00019 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.00019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 204 | loss: 0.00019 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.00018\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 205 | loss: 0.00018 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.00018\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 206 | loss: 0.00018 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.00018\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 207 | loss: 0.00018 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.00018\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 208 | loss: 0.00018 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.00018\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 209 | loss: 0.00018 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.00017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 210 | loss: 0.00017 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.00017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 211 | loss: 0.00017 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.00017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 212 | loss: 0.00017 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.00017\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 213 | loss: 0.00017 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.00017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 214 | loss: 0.00017 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.00017\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 215 | loss: 0.00017 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.00016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 216 | loss: 0.00016 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.00016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 217 | loss: 0.00016 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.00016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 218 | loss: 0.00016 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.00016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 219 | loss: 0.00016 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.00016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 220 | loss: 0.00016 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.00016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 221 | loss: 0.00016 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 222 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 223 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 224 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 225 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 226 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 227 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 228 | loss: 0.00015 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 229 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 230 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 231 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 232 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 233 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 234 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.00014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 235 | loss: 0.00014 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 236 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 237 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 238 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 239 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 240 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 241 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 242 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.00013\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 243 | loss: 0.00013 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 244 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 245 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 246 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 247 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 248 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 249 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 250 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.00012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 251 | loss: 0.00012 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 252 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 253 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 254 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 255 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 256 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 257 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 258 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 259 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.00011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 260 | loss: 0.00011 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 261 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 262 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 263 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 264 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 265 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 266 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 267 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 268 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 269 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.00010\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 270 | loss: 0.00010 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 271 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 272 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 273 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 274 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 275 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 276 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 277 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 278 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 279 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 280 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.00009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 281 | loss: 0.00009 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 282 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 283 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 284 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 285 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 286 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 287 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 288 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 289 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 290 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 291 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 292 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 293 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.00008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 294 | loss: 0.00008 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 295 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 296 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 297 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 298 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 299 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 300 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 301 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 302 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 303 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 304 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 305 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 306 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 307 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.00007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 308 | loss: 0.00007 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 309 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 310 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 311 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 312 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 313 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 314 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 315 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 316 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 317 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 318 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 319 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 320 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 321 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 322 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 323 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 324 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.00006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 325 | loss: 0.00006 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 326 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 327 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 328 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 329 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 330 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 331 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 332 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 333 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 334 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 335 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 336 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 337 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 338 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 339 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 340 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 341 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 342 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 343 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 344 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.00005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 345 | loss: 0.00005 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 346 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 347 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 348 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 349 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 350 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 351 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 352 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 353 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 354 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 355 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 356 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 357 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 358 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 359 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 360 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 361 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 362 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 363 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 364 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 365 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 366 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 367 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 368 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 369 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.00004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 370 | loss: 0.00004 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 371 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 372 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 373 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 374 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 375 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 376 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 377 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 378 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 379 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 380 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 381 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 382 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 383 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 384 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 385 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 386 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 387 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 388 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 389 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 390 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 391 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 392 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 393 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 394 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 395 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 396 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 397 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 398 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 399 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 400 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -0.0105307199433\n",
      "Running Iteration : 1\n",
      "---------------------------------\n",
      "Run id: 103T2L\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 401 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 402 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.00003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 403 | loss: 0.00003 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 404 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 405 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 406 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 407 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 408 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 409 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 410 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 411 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 412 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 413 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 414 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 415 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 416 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 417 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 418 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 419 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 420 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 421 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 422 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 423 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 424 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 425 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 426 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 427 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 428 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 429 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 430 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 431 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 432 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 433 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 434 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 435 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 436 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 437 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 438 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 439 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 440 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 441 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 442 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 443 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 444 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 445 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 446 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 447 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 448 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 449 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 450 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 451 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 452 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 453 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.00002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 454 | loss: 0.00002 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 455 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 456 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 457 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 458 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 459 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 460 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 461 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 462 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 463 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 464 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 465 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 466 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 467 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 468 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 469 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 470 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 471 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 472 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 473 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 474 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 475 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 476 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 477 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 478 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 479 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 480 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 481 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 482 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 483 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 484 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 485 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 486 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 487 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 488 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 489 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 490 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 491 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 492 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 493 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 494 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 495 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 496 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 497 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 498 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 499 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 500 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 501 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 502 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 503 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 504 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 505 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 506 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 507 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 508 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 509 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 510 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 511 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 512 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 513 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 514 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 515 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 516 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 517 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 518 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 519 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 520 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 521 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 522 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 523 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 524 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 525 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 526 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 527 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 528 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 529 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 530 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 531 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 532 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 533 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 534 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 535 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 536 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 537 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 538 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 539 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 540 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 541 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 542 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 543 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 544 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 545 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 546 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 547 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 548 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 549 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 550 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 551 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 552 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 553 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 554 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 555 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 556 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 557 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 558 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 559 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 560 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 561 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 562 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 563 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.00001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 564 | loss: 0.00001 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 565 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 566 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 567 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 568 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 569 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 570 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 571 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 572 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 573 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 574 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 575 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 576 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 577 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 578 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 579 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 580 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 581 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 582 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 583 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 584 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 585 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 586 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 587 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 588 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 589 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 590 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 591 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 592 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 593 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 594 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 595 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 596 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 597 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 598 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 599 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 600 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -0.00367363348603\n",
      "Running Iteration : 2\n",
      "---------------------------------\n",
      "Run id: C570E6\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 601 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 602 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 603 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 604 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 605 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 606 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 607 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 608 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 609 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 610 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 611 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 612 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 613 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 614 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 615 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 616 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 617 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 618 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 619 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 620 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 621 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 622 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 623 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 624 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 625 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 626 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 627 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 628 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 629 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 630 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 631 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 632 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 633 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 634 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 635 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 636 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 637 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 638 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 639 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 640 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 641 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 642 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 643 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 644 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 645 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 646 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 647 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 648 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 649 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 650 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 651 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 652 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 653 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 654 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 655 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 656 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 657 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 658 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 659 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 660 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 661 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 662 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 663 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 664 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 665 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 666 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 667 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 668 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 669 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 670 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 671 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 672 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 673 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 674 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 675 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 676 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 677 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 678 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 679 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 680 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 681 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 682 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 683 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 684 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 685 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 686 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 687 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 688 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 689 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 690 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 691 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 692 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 693 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 694 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 695 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 696 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 697 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 698 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 699 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 700 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 701 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 702 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 703 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 704 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 705 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 706 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 707 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 708 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 709 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 710 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 711 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 712 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 713 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 714 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 715 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 716 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 717 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 718 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 719 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 720 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 721 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 722 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 723 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 724 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 725 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 726 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 727 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 728 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 729 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 730 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 731 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 732 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 733 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 734 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 735 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 736 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 737 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 738 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 739 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 740 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 741 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 742 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 743 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 744 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 745 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 746 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 747 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 748 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 749 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 750 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 751 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 752 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 753 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 754 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 755 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 756 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 757 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 758 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 759 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 760 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 761 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 762 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 763 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 764 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 765 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 766 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 767 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 768 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 769 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 770 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 771 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 772 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 773 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 774 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 775 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 776 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 777 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 778 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 779 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 780 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 781 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 782 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 783 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 784 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 785 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 786 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 787 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 788 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 789 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 790 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 791 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 792 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 793 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 794 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 795 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 796 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 797 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 798 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 799 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 800 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -0.00132188498508\n",
      "Running Iteration : 3\n",
      "---------------------------------\n",
      "Run id: 92FU33\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.021s\n",
      "| SGD | epoch: 801 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 802 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 803 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 804 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 805 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 806 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 807 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 808 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 809 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 810 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 811 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 812 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 813 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 814 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 815 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 816 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 817 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 818 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 819 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 820 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 821 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 822 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 823 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 824 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 825 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 826 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 827 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 828 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 829 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 830 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 831 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 832 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 833 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 834 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 835 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 836 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 837 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 838 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 839 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 840 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 841 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 842 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 843 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 844 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 845 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 846 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 847 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 848 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 849 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 850 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 851 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 852 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 853 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 854 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 855 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 856 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 857 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 858 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 859 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 860 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 861 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 862 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 863 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 864 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 865 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 866 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 867 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 868 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 869 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 870 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 871 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 872 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 873 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 874 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 875 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 876 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 877 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 878 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 879 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 880 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 881 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 882 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 883 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 884 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 885 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 886 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 887 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 888 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 889 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 890 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 891 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 892 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 893 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 894 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 895 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 896 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 897 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 898 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 899 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 900 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 901 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 902 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 903 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 904 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 905 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 906 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 907 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 908 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 909 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 910 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 911 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 912 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 913 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 914 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 915 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 916 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 917 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 918 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 919 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 920 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 921 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 922 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 923 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 924 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 925 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 926 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 927 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 928 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 929 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 930 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 931 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 932 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 933 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 934 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 935 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 936 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 937 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 938 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 939 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 940 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 941 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 942 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 943 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 944 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 945 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 946 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 947 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 948 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 949 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 950 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 951 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 952 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 953 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 954 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 955 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 956 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 957 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 958 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 959 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 960 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 961 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 962 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 963 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 964 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 965 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 966 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 967 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 968 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 969 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 970 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 971 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 972 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 973 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 974 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 975 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 976 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 977 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 978 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 979 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 980 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 981 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 982 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 983 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 984 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 985 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 986 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 987 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 988 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 989 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 990 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 991 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 992 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 993 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 994 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 995 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 996 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 997 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 998 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 999 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1000 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -0.000481540318578\n",
      "Running Iteration : 4\n",
      "---------------------------------\n",
      "Run id: 1RXD16\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 1001 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1002 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1003 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1004 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1005 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1006 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1007 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1008 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1009 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1010 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1011 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1012 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1013 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1014 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1015 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1016 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1017 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1018 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1019 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1020 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1021 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1022 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1023 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1024 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1025 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1026 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1027 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1028 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1029 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1030 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1031 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1032 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1033 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1034 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1035 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1036 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1037 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1038 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1039 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1040 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1041 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1042 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1043 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1044 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1045 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1046 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1047 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1048 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1049 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1050 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1051 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1052 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1053 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1054 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1055 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1056 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1057 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1058 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1059 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1060 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1061 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1062 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1063 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1064 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1065 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1066 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1067 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1068 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1069 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1070 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1071 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1072 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1073 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1074 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1075 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1076 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1077 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1078 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1079 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1080 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1081 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1082 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1083 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1084 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1085 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1086 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1087 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1088 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1089 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1090 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1091 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1092 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1093 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1094 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1095 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1096 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1097 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1098 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1099 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1100 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1101 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1102 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1103 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1104 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1105 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1106 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1107 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1108 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1109 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1110 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1111 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1112 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1113 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1114 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1115 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1116 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1117 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1118 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1119 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1120 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1121 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1122 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1123 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1124 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1125 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1126 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1127 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1128 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1129 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1130 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1131 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1132 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1133 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1134 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1135 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1136 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1137 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1138 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1139 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1140 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1141 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1142 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1143 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1144 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1145 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1146 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1147 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1148 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1149 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1150 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1151 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1152 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1153 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1154 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1155 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1156 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1157 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1158 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1159 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1160 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1161 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1162 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1163 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1164 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1165 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1166 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1167 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1168 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1169 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1170 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1171 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1172 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1173 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1174 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1175 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1176 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1177 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1178 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1179 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1180 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1181 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1182 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1183 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1184 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1185 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1186 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1187 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1188 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1189 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1190 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1191 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1192 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1193 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1194 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1195 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1196 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1197 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1198 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1199 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1200 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -0.000176228638156\n",
      "Running Iteration : 5\n",
      "---------------------------------\n",
      "Run id: PK68J6\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.023s\n",
      "| SGD | epoch: 1201 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1202 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1203 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1204 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1205 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1206 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1207 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1208 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1209 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1210 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1211 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1212 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1213 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1214 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1215 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1216 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1217 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 1218 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1219 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1220 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1221 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1222 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1223 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1224 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| SGD | epoch: 1225 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1226 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1227 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1228 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1229 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1230 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1231 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1232 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1233 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1234 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1235 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1236 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1237 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1238 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1239 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1240 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1241 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1242 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1243 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1244 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1245 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1246 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1247 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1248 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1249 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1250 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1251 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1252 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1253 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1254 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1255 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1256 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1257 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1258 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1259 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1260 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1261 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1262 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1263 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1264 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1265 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1266 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1267 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1268 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1269 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1270 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1271 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1272 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1273 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1274 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1275 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1276 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1277 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1278 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1279 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1280 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1281 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1282 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1283 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1284 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1285 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1286 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1287 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1288 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1289 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1290 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1291 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1292 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1293 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1294 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1295 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1296 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1297 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1298 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1299 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1300 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1301 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1302 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1303 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1304 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1305 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1306 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1307 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1308 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1309 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1310 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1311 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1312 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1313 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1314 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1315 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1316 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1317 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1318 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1319 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1320 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1321 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1322 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1323 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1324 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1325 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1326 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1327 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1328 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1329 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1330 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1331 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1332 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1333 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1334 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1335 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1336 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1337 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1338 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1339 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1340 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1341 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1342 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1343 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1344 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1345 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1346 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1347 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1348 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1349 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1350 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1351 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1352 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1353 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1354 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1355 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1356 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1357 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1358 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1359 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1360 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1361 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1362 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1363 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1364 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1365 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1366 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1367 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1368 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1369 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1370 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1371 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1372 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1373 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1374 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1375 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1376 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1377 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1378 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1379 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1380 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1381 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1382 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1383 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1384 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1385 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1386 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1387 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1388 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1389 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1390 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1391 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1392 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1393 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1394 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1395 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1396 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1397 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1398 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1399 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1400 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -6.46043309825e-05\n",
      "Running Iteration : 6\n",
      "---------------------------------\n",
      "Run id: LYMQH6\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 2001  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 1401 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2002  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1402 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2003  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1403 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2004  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1404 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2005  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1405 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2006  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1406 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2007  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1407 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2008  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1408 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2009  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1409 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2010  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1410 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1411 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2012  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1412 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2013  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1413 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2014  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1414 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2015  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1415 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2016  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1416 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2017  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1417 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2018  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1418 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2019  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1419 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2020  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1420 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2021  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1421 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2022  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1422 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2023  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1423 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2024  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1424 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2025  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1425 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2026  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1426 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2027  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1427 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2028  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1428 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2029  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1429 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2030  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1430 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2031  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1431 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2032  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1432 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2033  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1433 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2034  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1434 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2035  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1435 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2036  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1436 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2037  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1437 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2038  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1438 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2039  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1439 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2040  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1440 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2041  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1441 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2042  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1442 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2043  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1443 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2044  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1444 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2045  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1445 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2046  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1446 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2047  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1447 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2048  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1448 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2049  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1449 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2050  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1450 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2051  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1451 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2052  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1452 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2053  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1453 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2054  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1454 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2055  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1455 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2056  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1456 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2057  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1457 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2058  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1458 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2059  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1459 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2060  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1460 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2061  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1461 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2062  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1462 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2063  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1463 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2064  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1464 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2065  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1465 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2066  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1466 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2067  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1467 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2068  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1468 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2069  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1469 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2070  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1470 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2071  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1471 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2072  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1472 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2073  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1473 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2074  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1474 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2075  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1475 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2076  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1476 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2077  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1477 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2078  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1478 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2079  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1479 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2080  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1480 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2081  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1481 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2082  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1482 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1483 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2084  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1484 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2085  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1485 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2086  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1486 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2087  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1487 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2088  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1488 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2089  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1489 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2090  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1490 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2091  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1491 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2092  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1492 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2093  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1493 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2094  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1494 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2095  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1495 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2096  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1496 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2097  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1497 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1498 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1499 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1500 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1501 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1502 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1503 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1504 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1505 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1506 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1507 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1508 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1509 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1510 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1511 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1512 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1513 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1514 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1515 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1516 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1517 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1518 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1519 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1520 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1521 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1522 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1523 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1524 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1525 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1526 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1527 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1528 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1529 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1530 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1531 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1532 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1533 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1534 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1535 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1536 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1537 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1538 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1539 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1540 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1541 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1542 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1543 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1544 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1545 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1546 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1547 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1548 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1549 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1550 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1551 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2152  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1552 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1553 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1554 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1555 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1556 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1557 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2158  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1558 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1559 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2160  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1560 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2161  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1561 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2162  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1562 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2163  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1563 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2164  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1564 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2165  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1565 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2166  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1566 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2167  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1567 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2168  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1568 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2169  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1569 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2170  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1570 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2171  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1571 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2172  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1572 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2173  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1573 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2174  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1574 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2175  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1575 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2176  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1576 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2177  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1577 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2178  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1578 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2179  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1579 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2180  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1580 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2181  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1581 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2182  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1582 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2183  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1583 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2184  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1584 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2185  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1585 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2186  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1586 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2187  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1587 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2188  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1588 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2189  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1589 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2190  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1590 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2191  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1591 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2192  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1592 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2193  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1593 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2194  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1594 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2195  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1595 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2196  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1596 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2197  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1597 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2198  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1598 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2199  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1599 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1600 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -2.36984211369e-05\n",
      "Running Iteration : 7\n",
      "---------------------------------\n",
      "Run id: VSQ33P\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 2201  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 1601 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2202  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1602 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2203  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1603 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2204  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1604 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2205  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1605 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2206  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1606 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2207  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1607 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2208  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1608 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2209  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1609 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2210  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1610 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2211  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1611 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2212  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1612 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2213  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1613 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2214  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1614 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2215  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1615 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2216  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1616 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2217  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1617 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2218  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1618 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2219  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1619 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2220  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1620 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2221  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1621 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2222  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1622 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2223  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1623 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2224  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1624 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2225  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1625 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2226  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1626 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2227  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1627 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2228  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1628 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2229  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1629 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2230  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1630 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2231  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1631 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2232  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1632 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2233  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1633 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2234  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1634 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2235  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1635 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2236  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1636 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2237  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1637 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2238  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1638 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2239  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1639 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2240  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1640 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2241  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1641 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2242  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1642 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2243  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1643 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2244  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1644 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2245  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1645 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2246  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1646 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2247  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1647 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2248  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1648 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2249  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1649 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2250  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1650 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2251  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1651 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2252  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1652 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2253  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1653 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2254  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1654 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2255  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1655 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2256  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1656 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2257  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1657 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2258  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1658 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2259  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1659 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2260  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1660 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2261  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1661 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2262  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1662 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2263  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1663 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2264  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1664 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2265  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1665 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2266  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1666 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2267  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1667 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2268  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1668 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2269  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1669 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2270  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1670 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2271  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1671 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2272  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1672 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2273  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1673 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2274  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1674 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2275  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1675 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2276  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1676 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2277  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1677 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2278  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1678 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2279  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1679 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2280  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1680 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2281  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1681 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2282  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1682 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2283  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1683 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2284  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1684 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2285  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1685 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2286  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1686 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2287  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1687 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2288  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1688 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2289  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1689 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2290  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1690 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2291  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1691 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2292  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1692 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2293  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1693 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2294  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1694 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2295  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1695 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2296  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1696 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2297  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1697 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2298  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1698 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2299  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1699 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2300  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1700 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2301  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1701 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2302  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1702 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2303  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1703 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2304  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1704 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2305  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1705 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2306  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1706 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2307  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1707 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2308  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1708 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2309  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1709 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2310  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1710 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2311  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1711 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2312  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1712 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2313  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1713 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2314  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1714 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2315  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1715 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2316  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1716 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2317  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1717 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2318  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1718 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2319  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1719 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2320  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1720 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2321  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1721 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2322  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1722 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2323  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1723 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2324  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1724 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2325  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1725 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2326  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1726 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2327  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1727 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2328  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1728 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2329  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1729 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2330  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1730 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2331  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1731 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2332  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1732 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2333  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1733 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2334  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1734 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2335  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1735 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2336  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1736 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2337  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1737 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2338  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1738 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2339  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1739 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2340  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1740 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2341  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1741 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2342  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1742 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2343  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1743 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2344  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1744 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2345  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1745 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2346  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1746 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2347  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1747 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2348  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1748 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2349  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1749 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2350  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1750 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2351  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1751 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2352  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1752 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2353  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1753 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2354  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1754 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2355  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1755 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2356  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1756 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2357  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1757 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2358  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1758 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2359  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1759 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2360  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1760 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2361  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1761 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2362  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1762 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2363  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1763 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2364  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1764 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2365  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1765 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2366  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1766 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2367  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1767 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2368  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1768 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2369  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1769 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2370  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1770 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2371  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1771 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2372  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1772 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2373  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1773 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2374  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1774 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2375  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1775 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2376  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1776 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2377  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1777 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2378  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1778 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2379  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1779 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2380  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1780 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2381  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1781 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2382  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1782 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2383  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1783 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2384  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1784 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2385  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1785 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2386  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1786 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2387  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1787 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2388  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1788 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2389  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1789 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2390  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1790 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2391  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1791 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2392  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1792 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2393  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1793 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2394  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1794 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2395  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1795 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2396  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1796 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2397  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1797 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2398  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1798 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2399  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1799 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2400  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1800 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -8.69515690283e-06\n",
      "Running Iteration : 8\n",
      "---------------------------------\n",
      "Run id: SAQ7LQ\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 2401  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.022s\n",
      "| SGD | epoch: 1801 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2402  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1802 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2403  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1803 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2404  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1804 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2405  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1805 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2406  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1806 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2407  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1807 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2408  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1808 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2409  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1809 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2410  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1810 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2411  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1811 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2412  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1812 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2413  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1813 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2414  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1814 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2415  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1815 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2416  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1816 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2417  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1817 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2418  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1818 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2419  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1819 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2420  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1820 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2421  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1821 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2422  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1822 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2423  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1823 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2424  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1824 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2425  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1825 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2426  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1826 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2427  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1827 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2428  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1828 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2429  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1829 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2430  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1830 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2431  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1831 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2432  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1832 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2433  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1833 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2434  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1834 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2435  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1835 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2436  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1836 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2437  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1837 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2438  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1838 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2439  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1839 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2440  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1840 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2441  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1841 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2442  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1842 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2443  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1843 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2444  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1844 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2445  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1845 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2446  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1846 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2447  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1847 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2448  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1848 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2449  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1849 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2450  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1850 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2451  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1851 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2452  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1852 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2453  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1853 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2454  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1854 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2455  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1855 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2456  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1856 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2457  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1857 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2458  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1858 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2459  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1859 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2460  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1860 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2461  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1861 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2462  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1862 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2463  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1863 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2464  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1864 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2465  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1865 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2466  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1866 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2467  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1867 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2468  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1868 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2469  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1869 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2470  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1870 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2471  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1871 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2472  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1872 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2473  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1873 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2474  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1874 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2475  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1875 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2476  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1876 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2477  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1877 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2478  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1878 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2479  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1879 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2480  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1880 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2481  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1881 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2482  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1882 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2483  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1883 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2484  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1884 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2485  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1885 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2486  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1886 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2487  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1887 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2488  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1888 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1889 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1890 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1891 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1892 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1893 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2494  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1894 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2495  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1895 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2496  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1896 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2497  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1897 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2498  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1898 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1899 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1900 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2501  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1901 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2502  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1902 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1903 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1904 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1905 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2506  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1906 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2507  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1907 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2508  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1908 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2509  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1909 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2510  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1910 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2511  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1911 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2512  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1912 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2513  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1913 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2514  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1914 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2515  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1915 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2516  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1916 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2517  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1917 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2518  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1918 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2519  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1919 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2520  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1920 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2521  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1921 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2522  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1922 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2523  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1923 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2524  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1924 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2525  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1925 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2526  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1926 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2527  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1927 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2528  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1928 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2529  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1929 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2530  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1930 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2531  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1931 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2532  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1932 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2533  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1933 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2534  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1934 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2535  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1935 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2536  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1936 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2537  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1937 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2538  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1938 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2539  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1939 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2540  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1940 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2541  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1941 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2542  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1942 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2543  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1943 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2544  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1944 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2545  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1945 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2546  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1946 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2547  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1947 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2548  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1948 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2549  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1949 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2550  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1950 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2551  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1951 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2552  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1952 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2553  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1953 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2554  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1954 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2555  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1955 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2556  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1956 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2557  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1957 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2558  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1958 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2559  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1959 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2560  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1960 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2561  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1961 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2562  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1962 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2563  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1963 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2564  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1964 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2565  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.005s\n",
      "| SGD | epoch: 1965 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2566  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1966 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2567  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1967 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2568  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1968 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2569  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1969 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2570  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1970 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2571  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1971 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2572  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1972 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2573  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1973 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2574  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1974 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2575  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1975 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2576  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1976 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2577  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1977 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2578  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1978 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1979 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1980 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2581  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1981 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2582  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1982 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2583  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1983 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2584  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1984 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2585  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1985 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2586  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1986 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1987 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1988 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1989 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1990 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1991 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1992 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1993 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 1994 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1995 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1996 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1997 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 1998 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 1999 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2000 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -3.19059886351e-06\n",
      "Running Iteration : 9\n",
      "---------------------------------\n",
      "Run id: ZJMO98\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 220\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.024s\n",
      "| SGD | epoch: 2001 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2002 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2003 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2004 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2005 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2006 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2007 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2008 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2009 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2010 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2011 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2012 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2013 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2014 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2015 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2016 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2017 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2018 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2019 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2020 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2021 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2022 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2023 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2024 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2025 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2026 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2027 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2028 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2029 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2030 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2031 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2032 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2033 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2034 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2035 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2036 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2037 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2038 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2039 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2040 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2041 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2042 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2043 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2044 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2045 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2046 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2047 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2048 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2049 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2050 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2051 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2052 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2053 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2054 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2055 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2056 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2057 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2058 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2059 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2060 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2061 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2062 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2063 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2064 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2065 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2066 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2067 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2068 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2069 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2070 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2071 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2072 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2073 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2074 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2075 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2076 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2077 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2078 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2079 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2080 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2081 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2082 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2083 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2084 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2085 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2086 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2087 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2088 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2089 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2090 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2091 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2092 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2093 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2094 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2095 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2096 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2097 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2098 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2099 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2100 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2101 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2102 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2103 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2104 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2105 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2106 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2107 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2108 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2109 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2110 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2111 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2112 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2713  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2113 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2714  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2114 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2715  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2115 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2716  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2116 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2717  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2117 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2718  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2118 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2719  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2119 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2720  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2120 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2721  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2121 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2722  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2122 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2723  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2123 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2724  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2124 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2725  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2125 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2726  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2126 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2727  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 2127 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2728  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2128 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2729  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2129 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2730  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2130 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2731  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2131 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2732  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2132 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2733  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2133 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2734  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2134 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2735  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2135 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2736  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2136 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2737  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2137 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2738  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2138 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2739  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2139 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2740  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2140 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2741  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2141 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2742  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2142 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2743  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2143 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2744  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2144 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2745  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2145 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2746  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2146 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2747  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2147 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2748  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2148 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2149 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2150 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2751  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2151 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2752  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2152 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2753  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2153 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2754  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2154 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2755  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2155 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2756  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2156 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2757  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2157 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2758  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2158 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2759  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2159 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2760  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2160 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2761  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2161 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2762  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2162 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2763  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2163 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2764  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2164 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2765  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2165 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2766  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2166 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2767  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2167 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2768  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2168 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2769  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2169 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2770  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2170 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2771  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2171 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2772  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2172 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2773  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2173 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2774  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2174 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2775  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2175 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2776  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2176 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2777  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2177 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2778  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2178 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2779  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2179 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2780  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2180 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2781  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2181 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2782  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2182 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2783  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2183 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2784  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2184 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2785  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2185 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2786  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 2186 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2787  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2187 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2788  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2188 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2789  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 2189 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2790  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| SGD | epoch: 2190 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2791  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2191 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2192 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2193 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2794  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2194 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2795  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2195 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2796  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2196 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2797  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2197 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2798  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2198 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2799  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| SGD | epoch: 2199 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "Training Step: 2800  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| SGD | epoch: 2200 | loss: 0.00000 - binary_acc: 0.0000 -- iter: 220/220\n",
      "--\n",
      "rho -1.17079380743e-06\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "(256, 4)\n",
      "(4, 1)\n",
      "(220, 256)\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<type 'numpy.ndarray'>\n",
      "float32\n",
      "(220, 1)\n",
      "(11, 1)\n",
      "Tensor(\"MatMul_1:0\", shape=(220, 1), dtype=float32)\n",
      "Tensor(\"MatMul_3:0\", shape=(11, 1), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAugAAAITCAYAAABVD+lKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZckSZIqYoIuSZIkVcQEXZIkSarIvyjxpBOITfXRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1117e5b90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 265,
       "width": 372
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tflearn import DNN\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression, oneClassNN\n",
    "from tflearn.metrics import binary_accuracy_op\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "# Clear all the graph variables created in previous run and start fresh\n",
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()\n",
    "# Training examples\n",
    "X = data_train\n",
    "# Y = [[0], [0], [0], [0]]\n",
    "Y = targets_train\n",
    "# Y = list(Y)\n",
    "Y = Y.tolist()\n",
    "Y = [[i] for i in Y]\n",
    "\n",
    "# For testing the algorithm\n",
    "X_test = data_test\n",
    "Y_test = targets_test\n",
    "Y_test = Y_test.tolist()\n",
    "Y_test = [[i] for i in Y_test]\n",
    "\n",
    "m, n = data_train.shape\n",
    "No_of_inputNodes = n\n",
    "No_of_hiddenNodes = n\n",
    "print \"No_of_hiddenNodes\", No_of_hiddenNodes\n",
    "\n",
    "# Histogram of train, test\n",
    "# AUC: computation test;\n",
    "\n",
    "input_layer = input_data(shape=[None, No_of_inputNodes])  # input layer of size 2\n",
    "hidden_layer = fully_connected(input_layer, 4, bias=False, activation='sigmoid', name=\"hiddenLayer_Weights\",\n",
    "                               weights_init=\"normal\")  # hidden layer of size 2\n",
    "output_layer = fully_connected(hidden_layer, 1, bias=False, activation='linear', name=\"outputLayer_Weights\",\n",
    "                               weights_init=\"normal\")  # output layer of size 1\n",
    "\n",
    "## Initialize the weights with random.normal and seed= 42\n",
    "# hidden_layer.W = tflearn.initializations.normal(mean=0.0, stddev=1.0, dtype=tf.float32, seed=42)\n",
    "# output_layer.W = tflearn.initializations.normal(mean=0.0, stddev=1.0, dtype=tf.float32, seed=42)\n",
    "\n",
    "# assign the learnt weights\n",
    "wStar = hidden_layer.W\n",
    "VStar = output_layer.W\n",
    "\n",
    "# Hyper parameters for the one class Neural Network\n",
    "v = 0.04\n",
    "import tflearn.variables as va\n",
    "\n",
    "value = 0.5\n",
    "init = tf.constant_initializer(value)\n",
    "rho = va.variable(name='rho', dtype=tf.float32, shape=[], initializer=init)\n",
    "# rho_previous = va.variable(name='rho_previous',dtype=tf.float32,shape=[] )\n",
    "# rho_next = va.variable(name='rho_next', dtype=tf.float32,shape=[])\n",
    "rcomputed = []\n",
    "auc = []\n",
    "# rho=0.3\n",
    "oneClassNN = oneClassNN(output_layer, v, rho, hidden_layer, output_layer, optimizer='sgd', loss='OneClassNN_Loss',\n",
    "                        learning_rate=5)\n",
    "model = DNN(oneClassNN, tensorboard_verbose=3)\n",
    "model.fit(X, Y, n_epoch=200, show_metric=True)\n",
    "y_pred = model.predict(data_train)  # Apply some ops\n",
    "rho = np.percentile(y_pred, v * 100)\n",
    "rcomputed.append(rho)\n",
    "# # Define the Iteration for optimising and stabilizing the value of r\n",
    "iterStep = 0\n",
    "\n",
    "while (iterStep < 10):\n",
    "    print \"Running Iteration :\", iterStep\n",
    "    y_pred = model.predict(data_train)  # Apply some ops\n",
    "    #     y_pred = np.sort(y_pred) # Sort in ascending order\n",
    "    #     rhoIndex = int(v * len(data_train))\n",
    "    #   rho=(y_pred[rhoIndex] + y_pred[rhoIndex+1])/2\n",
    "    rho = np.percentile(y_pred, v * 100)\n",
    "    rStar = rho\n",
    "\n",
    "    #     rho_next =rho\n",
    "    model.fit(X, Y, n_epoch=200, show_metric=True, batch_size=220)\n",
    "    iterStep = iterStep + 1\n",
    "    rcomputed.append(rho)\n",
    "    print \"rho\", rho\n",
    "\n",
    "# The data_train and data_test\n",
    "data_train = data[0:220]\n",
    "y_predTrain = model.predict(data_train)  # Apply some ops\n",
    "\n",
    "data_test = data[220:231]\n",
    "y_predTest = model.predict(data_test)\n",
    "\n",
    "## PLot the AUC for the data\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# train_data, test_data, train_target, test_target = train_test_split(data, target, train_size = 0.8)\n",
    "# preds = model.predict(test_data)\n",
    "# targs = test_target\n",
    "\n",
    "# oneClass_nn_score = metrics.accuracy_score(targs, preds.round())\n",
    "# # Compute the AUC for OneClassNN\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "# OCNN_auc_score = metrics.auc(fpr, tpr)\n",
    "# auc.append(OCNN_auc_score)\n",
    "\n",
    "print type(wStar)\n",
    "print type(VStar)\n",
    "# # print \"Value of R computed is... \",rcomputed\n",
    "# # Make a figure\n",
    "# fig1 = plt.figure()\n",
    "\n",
    "# plt.hist(y_predTrain,  alpha=0.5, label='normal')\n",
    "# plt.hist(y_predTest,  alpha=0.5, label='anomalies')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.xlabel(\"Samples\")\n",
    "# plt.ylabel(\"Y_hat(Predicted Value)\")\n",
    "# plt.title(\"OneClass-NN(Output=Y_hat) Vs Samples\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot lines 1-3\n",
    "# fig2 = plt.figure()\n",
    "# line1 = plt.plot(y_predTrain-rcomputed[0],'bo-',label='Normal')\n",
    "# line2 = plt.plot(y_predTest-rcomputed[0],'go-',label='Anomalies')\n",
    "# plt.title(\"Anomaly-Score(Yhat-r)\")\n",
    "# plt.xlabel(\"Samples\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.legend(loc='lower right')\n",
    "# # Display the figure\n",
    "# plt.show()\n",
    "\n",
    "# # AUC Score computed for the data points\n",
    "# fig3 = plt.figure()\n",
    "# line1 = plt.plot(auc,'ro-',label='OneClass-NN')\n",
    "# plt.title(\"AUC Curve\")\n",
    "# # Display the figure\n",
    "# plt.show()\n",
    "\n",
    "print type(wStar)\n",
    "print type(VStar)\n",
    "print wStar.shape\n",
    "print VStar.shape\n",
    "print data_train.shape\n",
    "print wStar.dtype\n",
    "print VStar.dtype\n",
    "import tflearn\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "X_test = data_test\n",
    "X_test = X_test.astype(np.float32)\n",
    "print type(X)\n",
    "print X.dtype\n",
    "g = lambda x: x\n",
    "\n",
    "\n",
    "def nnScore(X, w, V, g):\n",
    "    return tf.matmul(g((tf.matmul(X, w))), V)\n",
    "\n",
    "\n",
    "nu = 0.04\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# k = tf.placeholder(tf.float32, shape=(None, 256))\n",
    "# Make a normal distribution, with a shifting mean\n",
    "train = nnScore(X, wStar, VStar, g)\n",
    "test = nnScore(X_test, wStar, VStar, g)\n",
    "# # Record that distribution into a histogram summary\n",
    "# tf.summary.histogram('r_____%1.6f' % rStar, train)\n",
    "# tf.summary.histogram('r_____%1.6f' % rStar, test)\n",
    "\n",
    "# # Setup a session and summary writer\n",
    "\n",
    "# writer = tf.summary.FileWriter(\"/tmp/histogram_example\")\n",
    "\n",
    "# summaries = tf.summary.merge_all()\n",
    "sess = tf.Session()\n",
    "print (train.shape)\n",
    "print (test.shape)\n",
    "print train\n",
    "print test\n",
    "# tflearn.helpers.summarizer.summarize_variables (train_vars=train, summary_collection='tflearn_summ')\n",
    "# tflearn.helpers.summarizer.summarize_variables (train_vars=test, summary_collection='tflearn_summ')\n",
    "sess.run(tf.initialize_all_variables())\n",
    "# plt.xlim(-4e-8,8e-8)\n",
    "arrayTrain = train.eval(session=sess)\n",
    "arrayTest = test.eval(session=sess)\n",
    "plt.hist(arrayTrain, label='Normal');\n",
    "plt.hist(arrayTest , label='Anomalies');\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('r = %1.6f' % rStar)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Setup a loop and write the summaries to disk\n",
    "# N = 1\n",
    "# for step in range(N):\n",
    "#   k_val = step/float(N)\n",
    "#   summ = sess.run(summaries,feed_dict={k: k_val})\n",
    "#   writer.add_summary(summ, global_step=step)\n",
    "\n",
    "\n",
    "\n",
    "# print(rStar)\n",
    "# result = nnScore(X, wStar, VStar, g)\n",
    "# print type(result)\n",
    "# print result.shape\n",
    "# res1 = va.variable(name='res2', dtype=tf.float32,shape=[220,1])\n",
    "# with sess.as_default():\n",
    "#     tflearn.variables.set_value(res1, result)\n",
    "# print(np.percentile(nnScore(X, wStar, VStar, g), q = 100 * nu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAugAAAITCAYAAABVD+lKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZckSZIqYoIuSZIkVcQEXZIkSarIvyjxpBOITfXRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1146ddd90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 265,
       "width": 372
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(arrayTrain,label='Normal');\n",
    "plt.hist(arrayTest,label='Anomalies');\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('r = %1.6f' % rStar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "(256, 4)\n",
      "(4, 1)\n",
      "(220, 256)\n",
      "<dtype: 'float32_ref'>\n",
      "<dtype: 'float32_ref'>\n",
      "<type 'numpy.ndarray'>\n",
      "float32\n",
      "0.00549998465925\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(220, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable res2 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/raghav/anaconda/lib/python2.7/site-packages/tflearn/variables.py\", line 65, in variable\n    validate_shape=validate_shape)\n  File \"/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"<ipython-input-6-a9b8dafa9c1d>\", line 153, in <module>\n    res1 = va.variable(name='res2', dtype=tf.float32,shape=[220,1])\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d2f4f1dd3c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'res2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m220\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwStar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVStar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tflearn/variables.pyc\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, validate_shape, device, restore)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                            \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                            \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                            validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    662\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 664\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable res2 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/raghav/anaconda/lib/python2.7/site-packages/tflearn/variables.py\", line 65, in variable\n    validate_shape=validate_shape)\n  File \"/Users/raghav/anaconda/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"<ipython-input-6-a9b8dafa9c1d>\", line 153, in <module>\n    res1 = va.variable(name='res2', dtype=tf.float32,shape=[220,1])\n"
     ]
    }
   ],
   "source": [
    "print type(wStar)\n",
    "print type(VStar)\n",
    "print wStar.shape\n",
    "print VStar.shape\n",
    "print data_train.shape\n",
    "print wStar.dtype\n",
    "print VStar.dtype\n",
    "import tflearn\n",
    "X = X.astype(np.float32)\n",
    "print type(X)\n",
    "print X.dtype\n",
    "g  = lambda x : x\n",
    "\n",
    "def nnScore(X, w, V, g):\n",
    "    return tf.matmul(g((tf.matmul(X, w))),V)\n",
    "          \n",
    "nu = 0.04\n",
    "print(rStar)\n",
    "result = nnScore(X, wStar, VStar, g)\n",
    "print type(result)\n",
    "print result.shape\n",
    "res1 = va.variable(name='res2', dtype=tf.float32,shape=[220,1])\n",
    "tflearn.variables.set_value(res1, result)\n",
    "plt.hist(nnScore(data_train, wStar, VStar, g));\n",
    "plt.hist(nnScore(data_test, wStar, VStar, g));\n",
    "plt.title('r = %1.6f' % rStar)\n",
    "# print(np.percentile(nnScore(X, wStar, VStar, g), q = 100 * nu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter to MergeFrom() must be instance of same class: expected tensorflow.HistogramProto got numpy.ndarray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ffb3fc720d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/histogram_example'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'means'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhisto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Parameter to MergeFrom() must be instance of same class: expected tensorflow.HistogramProto got numpy.ndarray."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sess = tf.Session()\n",
    "means_placeholder = tf.placeholder(tf.float32)\n",
    "tf.summary.histogram('means', means_placeholder)\n",
    "summaries = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('/tmp/histogram_example')\n",
    "means = np.random.random(10)    \n",
    "writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='means', histo=means)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(220, 1)\n"
     ]
    }
   ],
   "source": [
    "# X = X.astype(np.float32)\n",
    "# Xtest = data_test\n",
    "# Xtest  = Xtest.astype(np.float32)\n",
    "result = nnScore(X, wStar, VStar, g)\n",
    "print type(result)\n",
    "print result.shape\n",
    "# import tensorflow as tf\n",
    "# sess = tf.InteractiveSession()\n",
    "# b = result.eval()\n",
    "# plt.hist(nnScore(X, wStar, VStar, g));\n",
    "# plt.hist(nnScore(Xtest, wStar, VStar, g));\n",
    "# plt.title('r = %1.6f' % rStar)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # y_pred = model.predict(data_test)\n",
    "# oneClass_nn_time = time() \n",
    "# preds = model.predict(X_test)\n",
    "# targs = targets_test\n",
    "\n",
    "# oneClass_nn_score = metrics.accuracy_score(targs, preds.round())  \n",
    "# # Compute the AUC for OneClassNN\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "# oneClass_nn_auc_score = metrics.auc(fpr, tpr)\n",
    "# # oneClass_nn_score = metrics.accuracy_score(targs, np.ones((len(targs),)))  \n",
    "# oneClass_nn_time = time() - oneClass_nn_time\n",
    "# print \"OneClassNN- Classifier Accuracy\",oneClass_nn_score,oneClass_nn_time\n",
    "# print \"OneClassNN- Classifier AUC:\",oneClass_nn_auc_score,oneClass_nn_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODELS: OneClass SVM, LINEAR KERNEL with Random Fourier Features, RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# set nu (which should be the proportion of outliers in our dataset)\n",
    "nu = float(outliers.shape[0]) / target.shape[0]  \n",
    "print(\"nu\", nu)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, pipeline\n",
    "from sklearn.kernel_approximation import (RBFSampler,\n",
    "                                          Nystroem)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics \n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "kernel_svm = svm.SVC(gamma=.2)\n",
    "linear_svm = svm.LinearSVC()\n",
    "oneClass_svm = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.00005)  \n",
    "# oneClass_svm = svm.OneClassSVM(nu=nu, kernel='linear', gamma=0.00005)\n",
    "# create pipeline from kernel approximation\n",
    "# and linear svm\n",
    "feature_map_fourier = RBFSampler(gamma=.2, random_state=1)\n",
    "fourier_approx_svm = pipeline.Pipeline([(\"feature_map\", feature_map_fourier),\n",
    "                                        (\"svm\", svm.LinearSVC())])\n",
    "\n",
    "OneClass_svm_Linear_fourier = pipeline.Pipeline([(\"feature_map\", feature_map_fourier),\n",
    "                                          (\"svm\", svm.OneClassSVM(nu=nu, kernel='linear', gamma=0.00005))])\n",
    "\n",
    "# fit and predict using linear and kernel svm:\n",
    "\n",
    "kernel_svm_time = time()\n",
    "kernel_svm.fit(data_train, targets_train)\n",
    "\n",
    "kernel_svm_score = kernel_svm.score(data_test, targets_test)\n",
    "kernel_svm_time = time() - kernel_svm_time\n",
    "\n",
    "## Compute AUC for the Linear SVM classifier performance\n",
    "preds = kernel_svm.predict(data_test)  \n",
    "targs = targets_test\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "kernel_svm_auc_score = metrics.auc(fpr, tpr)\n",
    "kernel_svm_time = time() - kernel_svm_time\n",
    "print \"kernel_svm Classifier AUC:\",kernel_svm_auc_score,kernel_svm_time\n",
    "\n",
    "print \"Kernel SVM-Targets and predicted..\"\n",
    "print targs\n",
    "print preds\n",
    "\n",
    "linear_svm_time = time()\n",
    "linear_svm.fit(data_train, targets_train)\n",
    "linear_svm_score = linear_svm.score(data_test, targets_test)\n",
    "preds = linear_svm.predict(data_train)  \n",
    "targs = targets_train\n",
    "\n",
    "## Compute AUC for the Linear SVM classifier performance\n",
    "preds = linear_svm.predict(data_train)  \n",
    "targs = targets_train\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "linear_svm_auc_score = metrics.auc(fpr, tpr)\n",
    "linear_svm_time = time() - linear_svm_time\n",
    "# print \"linear_svm Classifier AUC:\",linear_svm_auc_score,linear_svm_time\n",
    "\n",
    "\n",
    "oneClass_svm_time = time()\n",
    "oneClass_svm.fit(data_train) \n",
    "preds = oneClass_svm.predict(data_test)  \n",
    "targs = targets_test\n",
    "oneClass_svm_score = metrics.accuracy_score(targs, preds)  \n",
    "oneClass_svm_time = time() - oneClass_svm_time\n",
    "\n",
    "print \"oneClass_svm and predicted..\"\n",
    "print targs\n",
    "print preds\n",
    "\n",
    "# Compute the AUC for oneClass_svm\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "oneClass_svm_auc_score = metrics.auc(fpr, tpr)\n",
    "print \"oneClass_svm Classifier AUC:\",oneClass_svm_auc_score,oneClass_svm_time\n",
    "\n",
    "## Use Sampling for RFF method \n",
    "sample_sizes = 30 * np.arange(1, 10)\n",
    "fourier_scores = []\n",
    "fourier_times = []\n",
    "fourier_auc_scores= []\n",
    "\n",
    "for D in sample_sizes:\n",
    "    fourier_approx_svm.set_params(feature_map__n_components=D)\n",
    "    start = time()\n",
    "    fourier_approx_svm.fit(data_train, targets_train)\n",
    "    fourier_times.append(time() - start)\n",
    "\n",
    "    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n",
    "    fourier_scores.append(fourier_score)\n",
    "    \n",
    "    ## Compute AUC for the FF  classifier performance\n",
    "    preds = fourier_approx_svm.predict(data_test)  \n",
    "    targs = targets_test\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "    fourier_approx_auc_score = metrics.auc(fpr, tpr)\n",
    "    fourier_auc_scores.append(fourier_approx_auc_score)\n",
    "#     print \"fourier_approx_svm Classifier AUC:\",fourier_approx_auc_score,fourier_times\n",
    "\n",
    "\n",
    "## Use Sampling for OneClassSVM + Linear Kerner+ fouriier features method \n",
    "sample_sizes = 30 * np.arange(1, 10)\n",
    "OneClass_svm_Linear_fourtier_scores = []\n",
    "OneClass_svm_Linear_fourtier_times = []\n",
    "OneClass_svm_Linear_fourtier_auc_scores= []\n",
    "\n",
    "for D in sample_sizes:\n",
    "    OneClass_svm_Linear_fourier.set_params(feature_map__n_components=D)\n",
    "    start = time()\n",
    "    OneClass_svm_Linear_fourier.fit(data_train)\n",
    "    OneClass_svm_Linear_fourtier_times.append(time() - start)\n",
    "    preds = OneClass_svm_Linear_fourier.predict(data_test)  \n",
    "    targs = targets_test\n",
    "    \n",
    "    OneClass_svm_Linear_fourtier_score = metrics.accuracy_score(targs, preds)  \n",
    "    OneClass_svm_Linear_fourtier_scores.append(OneClass_svm_Linear_fourtier_score)\n",
    "    \n",
    "    ## Compute AUC for the FF  classifier performance\n",
    "#     preds = OneClass_svm_Linear_fourier.predict(data_train)  \n",
    "#     targs = targets_train\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(targs, preds)\n",
    "    OneClass_svm_Linear_fourtier_auc_score = metrics.auc(fpr, tpr)\n",
    "    OneClass_svm_Linear_fourtier_auc_scores.append(OneClass_svm_Linear_fourtier_auc_score)\n",
    "#     print \"fourier_approx_svm Classifier AUC:\",fourier_approx_auc_score,fourier_times\n",
    "\n",
    "print \"OneClass_svm_Linear_fourier and predicted...\"\n",
    "print targs\n",
    "print preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from data_fetch import prepare_usps_mlfetch\n",
    "# [Xtrue,Xlabels] = prepare_usps_mlfetch()\n",
    "# data = Xtrue\n",
    "# target = Xlabels\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# train_data, test_data, train_target, test_target = train_test_split(data, target, train_size = 0.8)\n",
    "# train_data.shape\n",
    "\n",
    "\n",
    "# # We learn the digits on the first half of the digits\n",
    "# data_train, targets_train = train_data,train_target\n",
    "# # Now predict the value of the digit on the second half:\n",
    "# data_test, targets_test = test_data,test_target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tflearn\n",
    "# var= tflearn.variables.get_all_trainable_variable()\n",
    "# print type(var)\n",
    "# for v in var:\n",
    "#     print v\n",
    "print preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fourier_auc_scores\n",
    "print OneClass_svm_Linear_fourtier_auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.figure(figsize=(8, 8))\n",
    "# accuracy = plt.subplot(111)\n",
    "# second y axis for timeings\n",
    "# timescale = plt.subplot(111)\n",
    "\n",
    "# print \"OneClassNN\",oneClass_nn_score,oneClass_nn_time\n",
    "# print\"oneClass_svm_score\", oneClass_svm_score,oneClass_svm_time\n",
    "# print\"linear_svm_score\", linear_svm_score,linear_svm_time\n",
    "# print \"kernel_svm_score\", kernel_svm_score,kernel_svm_time\n",
    "# print \"fourier_scores\", np.mean(fourier_scores),np.mean(fourier_times)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# basepath = \"/Users/raghav/Documents/Uni/ECML_2017/experiments/cifar_10/\"\n",
    "mse = np.zeros((1,5))\n",
    "\n",
    "# mse[:,0] = np.asarray(oneClass_svm_score)\n",
    "mse[:,0] = np.asarray(np.mean(OneClass_svm_Linear_fourtier_scores))\n",
    "mse[:,1] = np.asarray(oneClass_svm_score)\n",
    "\n",
    "\n",
    "\n",
    "mse[:,2] = np.asarray(kernel_svm_score)\n",
    "mse[:,3] = np.asarray(np.mean(fourier_scores))\n",
    "mse[:,4] = np.asarray(oneClass_nn_score)\n",
    "\n",
    "df = pd.DataFrame(mse, columns=['OC-SVM-LFF', 'OC-SVM-RBF','RBF-SVM', 'RFF-SVM','OneClass-NN'])\n",
    "df.plot.box()\n",
    "plt.title('Classifier Performance',fontsize=12,fontweight=\"bold\")\n",
    "plt.ylabel(\" Accuracy\",fontsize=12,fontweight=\"bold\")\n",
    "plt.xticks(rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the AUC Acore for various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.figure(figsize=(8, 8))\n",
    "# accuracy = plt.subplot(111)\n",
    "# second y axis for timeings\n",
    "# timescale = plt.subplot(111)\n",
    "\n",
    "# print \"OneClassNN\",oneClass_nn_score,oneClass_nn_time\n",
    "# print\"oneClass_svm_score\", oneClass_svm_score,oneClass_svm_time\n",
    "# print\"linear_svm_score\", linear_svm_score,linear_svm_time\n",
    "# print \"kernel_svm_score\", kernel_svm_score,kernel_svm_time\n",
    "# print \"fourier_scores\", np.mean(fourier_scores),np.mean(fourier_times)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# basepath = \"/Users/raghav/Documents/Uni/ECML_2017/experiments/cifar_10/\"\n",
    "mse = np.zeros((1,5))\n",
    "\n",
    "# mse[:,0] = np.asarray(oneClass_svm_auc_score)\n",
    "\n",
    "mse[:,0] = np.asarray(np.mean(OneClass_svm_Linear_fourtier_auc_scores))\n",
    "mse[:,1] = np.asarray(oneClass_svm_auc_score)\n",
    "mse[:,2] = np.asarray(kernel_svm_auc_score)\n",
    "mse[:,3] = np.asarray(np.mean(fourier_auc_scores))\n",
    "mse[:,4] = np.asarray(oneClass_nn_auc_score)\n",
    "\n",
    "df = pd.DataFrame(mse, columns=['OC-SVM-LFF', 'OC-SVM-RBF','RBF-SVM', 'RFF-SVM','OneClass-NN'])\n",
    "df.plot.box()\n",
    "plt.title('Classifier AUC Performance',fontsize=12,fontweight=\"bold\")\n",
    "plt.ylabel(\" AUC\",fontsize=12,fontweight=\"bold\")\n",
    "plt.xticks(rotation=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print \" Predictions of One Class NN\"\n",
    "# # print Y_test\n",
    "# # print preds\n",
    "\n",
    "\n",
    "xtestDF_OC_RBF = oneClass_svm.decision_function(X_test)\n",
    "xtestDF_OC_LFF = OneClass_svm_Linear_fourier.decision_function(X_test)\n",
    "\n",
    "print \" One Class SVM - Decision Functions\"\n",
    "print xtestDF_OC_RBF\n",
    "print Y_test\n",
    "print \" ##############################################\"\n",
    "print xtestDF_OC_LFF\n",
    "print Y_test\n",
    "## But how can we know the decision function for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PLotting the decision functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager\n",
    "# from sklearn import svm\n",
    "\n",
    "# nx, ny = (len(data), 256)\n",
    "# x = np.linspace(-1, 1, nx)\n",
    "# y = np.linspace(-1, 1, ny)\n",
    "# xx, yy =  np.meshgrid(x, y)\n",
    "\n",
    "# # xx, yy = np.meshgrid(np.linspace(-1, 1, 231), np.linspace(-1, 1, 231))\n",
    "# ### Synthetic Data\n",
    "# # # Generate train data\n",
    "# # X = 0.3 * np.random.randn(100, 2)\n",
    "# # X_train = np.r_[X + 2, X - 2]\n",
    "# # # Generate some regular novel observations\n",
    "# # X = 0.3 * np.random.randn(20, 2)\n",
    "# # X_test = np.r_[X + 2, X - 2]\n",
    "# # # Generate some abnormal novel observations\n",
    "# # X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# ## USPS Data\n",
    "# X_train = data[0:180]\n",
    "# X_test = data[181:219]\n",
    "# X_outliers = data[220:231]\n",
    "\n",
    "\n",
    "# # fit the model\n",
    "# clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "# clf.fit(X_train)\n",
    "# y_pred_train = clf.predict(X_train)\n",
    "# y_pred_test = clf.predict(X_test)\n",
    "# y_pred_outliers = clf.predict(X_outliers)\n",
    "# n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "# n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "# n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "# # plot the line, the points, and the nearest vectors to the plane\n",
    "# Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plt.title(\"Novelty Detection\")\n",
    "# plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "# a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "# plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "# b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "# b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "# c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "# plt.axis('tight')\n",
    "# plt.xlim((-1, 1))\n",
    "# plt.ylim((-1, 1))\n",
    "# plt.legend([a.collections[0], b1, b2, c],\n",
    "#            [\"learned frontier\", \"training observations\",\n",
    "#             \"new regular observations\", \"new abnormal observations\"],\n",
    "#            loc=\"upper left\",\n",
    "#            prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "# plt.xlabel(\n",
    "#     \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "#     \"errors novel abnormal: %d/40\"\n",
    "#     % (n_error_train, n_error_test, n_error_outliers))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
